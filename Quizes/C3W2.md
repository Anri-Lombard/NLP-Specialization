# RNNs for language Modeling

1. What would be the probability of a five word sequence using a penta-gram?
   - $P(w_5, w_4, w_3, w_2, w_1) = P(w_5|w_4, w_3, w_2, w_1)P(w_4|w_3, w_2, w_1)P(w_3|w_2, w_1)P(w_2|w_1)P(w_1)$
2. The number of parameters in an RNN is the same regardless of the input's length.
   - True
3. Select all the examples that correspond to a “many to one” architecture.
   - An RNN which inputs a sentence  and determines the sentiment.
   - An RNN which inputs a conversation and determines the topic.
4. What should be the size of matrix $W_h$, if $h^{<t>}$ had size 4x1 and $x^{<t>}$ 10x1? $h^{<t>} = g(W_h [h^{<t-1>}, x^{<t>}] + b_h)$
   - 4 x 14
5. In the next equation, why is there a division by the number of time steps but not one for the number of classification categories? $J = -\frac{1}{T} \sum_{t=1}^{T} \sum_{c=1}^{C} y_c^{<t>} \log(\hat{y}_c^{<t>})$
   - Because there is just one value in every vector $y^{<t>}$ different from 0.
6. What problem, related to vanilla RNNs,  do GRUs tackle?
   - Loss of relevant information for long sequences of words.
7. Bidirectional RNNs are acyclic graphs, which means that the computations in one direction are independent from the ones in the other direction.
   - True
8. Compared to Traditional Language models which of the following problems does an RNN help us with?
   - Helps us solve memory issues.
   - Helps us solve RAM issues.
9. What type of RNN structure would you use when implementing machine translation?
    - Many to Many
10. In the scan() function the variable cur_value corresponds to the hidden state in an RNN.
    - True
