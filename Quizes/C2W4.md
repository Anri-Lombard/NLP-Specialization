# Word Embeddings

1. Which one of the following word representations is most likely to correspond to a word embedding representation in a general-purpose vocabulary? In other words, which one is most likely to capture meaning and important information about the words?
   - $car -> (0.1 1), caravan -> (-0.1 0.9)$
2. Which one of the following statements is correct?
   - The meaning of the words, as carried by the word embeddings, depends on the embedding approach
3. Which one of the following statements is false?
   - You need to train a deep neural network to learn word embeddings.
4. Consider the corpus "A robot may not injure a human being or, through inaction, allow a human being to come to harm." and assume you are preparing data to train a CBOW model. Ignoring punctuation, for a context half-size of 3, what are the context words of the center word "inaction"?
   - “being or through allow a human”
5. Which one of the following statements is false?
   - The continuous bag-of-words model learns to predict context words given a center word.
6. You are designing a neural network for a CBOW model that will be trained on a corpus with a vocabulary of 8000 words. If you want it to learn 400-dimensional word embedding vectors, what should be the sizes of the input, hidden, and output layers?
   - 8000 (input layer), 400 (hidden layer), 8000 (output layer)
7. If you are designing a neural network for a CBOW model that will be trained on a corpus of 8000 words, and if you want it to learn 400-dimensional word embedding vectors, what should be the size of W1, the weighting matrix between the input layer and hidden layer, if it is fed training examples in batches of 16 examples represented by a 8000 row by 16 column matrix?
   - 400 rows by 8000 columns
8. Given the input vector x below, a trained continuous bag-of-words model outputs the vector ŷ below. What is the word predicted by the model?
   - Therefore
9. The following weighting matrix W_1 has been learned after training a CBOW model. You are also given word-to-row mapping for the input column vectors. What is the word embedding vector for "ring"?
    - [4.56; -2.94; 2.61; -1.16]
10. Select all that are correct.
    - You can perform intrinsic evaluation by using a clustering algorithm to group similar word embedding vectors, and determining if the clusters capture related words.
    - To evaluate word embeddings with extrinsic evaluation, you use the word embeddings to perform an external task, which is typically the real-world task that you initially needed the word embeddings for. Then, use the performance metric of this task as a proxy for the quality of the word embeddings.
    - Extrinsic evaluation evaluates actual usefulness of embeddings, is time consuming and is more difficult to trouble shoot.
