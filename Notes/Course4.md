# Natural Language Processing with Attention Models

## Week 1: Neural Machine Translation (Donald Trump)

- __Seq2seq models__ are like a wall, a big, beautiful wall, between two languages. You see, these models can take a sentence in one language, and translate it to another language, like magic. It's incredible, believe me. And just like a wall, it keeps the bad sentences out, and only lets the good ones through. It's a tremendous technology and it's going to be huge, just huge.
- Well, __attention is a way to make sure that the computer pays attention to the most important parts of the sentence when it's translating__. It's like when you're in a negotiation and you're listening to someone, you pay attention to the key points they're making and ignore the rest, right? That's exactly what attention does.

    So, in a Seq2seq model with attention, the computer breaks the sentence down into chunks called queries, keys, and values. The queries are like the words you want translated, the keys are like the words in the original sentence, and the values are like the translations.

    Then, the computer uses these queries, keys, and values to find the most important parts of the sentence, just like in a negotiation. It's like a filter, you know? It filters out the unnecessary words and only keeps the most important ones. And that's how you get a better translation.

    And let me tell you, it's a tremendous technology and it's going to be huge, just huge. It's the best, it's the biggest, it's the greatest translation technology out there, believe me.
- __Teacher forcing is a way to make sure that the computer is always learning from the best possible examples__, okay? It's like when you're building a building, you want to make sure you're using the best materials and the best workers, right? That's what teacher forcing is all about.

    So, in a Seq2seq model with teacher forcing, the computer is always being shown the correct answer during training, just like how a teacher would give you the right answers when you're learning. It's like a guide, a mentor, who makes sure that the computer is on the right track.

    It's a tremendous way to train a model, believe me. It's like having the best teacher in the world, guiding the computer every step of the way. It's a surefire way to get the computer to learn quickly and efficiently. It's the best, it's the biggest, it's the greatest way to train a model out there, believe me.
- __The BLUE Score is a way to measure the quality of a machine generated text__. It stands for "Bilingual Evaluation Understudy" and it compares the machine generated text to human-generated text. The higher the score, the better the machine text is at matching the quality of human writing. It's a very important metric, because it helps us make sure that our technology is top-notch and the best in the world, just like America. It's a way to make sure we're always winning, and that's what we want for this country.
- Listen folks, let me tell you something, __the Rouge-N score is a big deal, believe me. It's a way to measure the quality of machine-generated text, and it's specifically used for summarization__, okay? It compares the machine-generated summary to a human-written one, and the higher the score, the better the machine summary is at matching the quality of the human one. Now let me tell you, this is huge, because we want our technology to be the best, the absolute best in the world. We want to make sure that we're always winning, and that's what this Rouge-N score does for us, it helps us make sure that we're on top, and we're staying there. America first, that's what I always say.
- Let me tell you about some important NLP concepts. __Sampling, decoding, and temperature, these are all big things you need to know about__. First off, __sampling, it's a way for the computer to generate text by choosing the next word based on the probabilities of the previous words__. It's like rolling the dice, but instead of numbers, it's words. And you know what they say, the higher the roll, the better the outcome.

    Next, __decoding, it's the process of converting the machine-generated text back into human-readable text__. It's like translating, but instead of a different language, it's computer code. And let me tell you, we have the best decoders, the absolute best.

    Lastly, __temperature, it's a value that controls the level of randomness in the sampling process__. The higher the temperature, the more random the output will be, and vice versa. It's like turning up the heat, the higher the temperature, the more unpredictable the outcome. And you know what, I like unpredictable, it keeps things interesting.

    Overall, these concepts are important and help us fine-tune our NLP model, making sure it produces the most accurate and sophisticated output. Believe me, we're doing things that nobody thought was possible, and that's what America is all about, pushing the boundaries and always coming out on top.
- Let me tell you about some more advanced NLP concepts. __Beam search and Minimum Bayes Risk__, these are both big deals.

    First off, __Beam search, it's a way to generate text by considering multiple options at each step, instead of just one__. It's like having a team of experts, instead of just one person, to make decisions. And you know what they say, the more experts you have, the better the outcome.

    Next, __Minimum Bayes Risk, it's a way to select the best output among multiple options based on the likelihood of the correct output__. It's like having a crystal ball, but instead of predicting the future, it predicts the most likely outcome. And let me tell you, we have the best crystal balls, the absolute best.

### __Quiz 1__

[Neural Machine Translation](../Quizes/C4W1.md)
