# Text Summarization

1. Select all the correct answers.
   - With transformers, the vanishing gradient problem isn't related with length of the sequences because we have access to all word positions at all times.
   - Transformers are able to take more advantage from parallel computing than other RNN architectures previously covered in the course.
   - Even RNN architectures like GRUs and LSTMs don't work as well as transformers for really long sequences.
2. Which of the following are applications of transformers?
   - All of the above
3. What is one of the biggest techniques that the T5 model brings about?
   - It makes use of transfer learning and the same model could be used for several applications. This implies that other tasks could be used to learn information that would benefit us on different tasks. 
4. When it comes to translating french to english using dot product attention:
   - The intuition is that each query $q_i$, picks most similar key $k_j$. This allows the attention model to focus on the right words at each time step.
   - You find the distribution by multiplying the queries by the keys (you might need to scale), take the softmax and then multiply it by the values.
   - The queries are the english words and the keys and values are the french words.
5. Which of the following corresponds to the causal (self) attention mechanism?
   - In one sentence, words look at previous words (used for generation). They can not look ahead. 
6. Let's explore multi-headed attention in this problem. Select all that apply.
   - Each head learns a different linear transformations to represent words.
   - Those linear transformations are combined and run through a linear layer to give you the final representation of words.
   - Multi-Headed models attend to information from different representations at different positions
7. Which of the following is true about about bi-directional attention?
   - It could attend to words before and after the target word.
8. Why is there a residual connection around each attention layer followed by a layer normalization step in the in the decoder network?
   - To speed up the training, and significantly reduce the overall processing time.
9. The structure of the text input when implementing a summarization task is as follows:
    - ARTICLE TEXT <EOS> SUMMARY <EOS> <pad>
10. In the lecture, the way summarization is generated is using:
    - Next word generation.
