# Logistic Regression

1. When performing logistic regression on sentiment analysis, you represented each tweet as a vector of ones and zeros. However your model did not work well. Your training cost was reasonable, but your testing cost was just not acceptable. What could be a possible reason?
   - The vector representations are sparse and therefore it is much harder for your model to learn anything that could generalize well to the test set.

2. Which of the following are examples of text preprocessing?
   - Stemming, or the process of reducing a word to its word stem.
   - Lowercasing, which is the process of removing changing all capital letter to lower case.
   - Removing stopwords, punctuation, handles and URLs

3. The sigmoid function is defined as $h(x^{(i)}, \theta) = \frac{1}{1 + e^{-\theta^Tx^{(i)}}}$. Which of the following is true.
   - Large positive values of $\theta^Tx^{(i)}$ will make $h(x^{(i)}, \theta)$ closer to 1 and large negative values of $\theta^Tx^{(i)}$ will make $h(x^{(i)}, \theta)$ closer to 0.

4. The cost function for logistic regression is defined as $J(\theta) = \frac{1}{m}\sum_{i=1}^{m}[y^{(i)}log(h(x^{(i)}, \theta)) + (1 - y^{(i)})log(1 - h(x^{(i)}, \theta))]$. Which of the following is true about the cost function above. Mark all the correct ones.
   - When $y^{(i)} = 1$, as $h(x^{(i)}, \theta)$ approaches 0, the cost function approaches infinity.
   - When $y^{(i)} = 0$, as $h(x^{(i)}, \theta)$ approaches 0, the cost function approaches 0.

5. For what value of $\theta^Tx^{(i)}$ will $h(x^{(i)}, \theta)$ be 0.5?
   - $\theta^Tx^{(i)} = 0$

6. Select all that apply. When performing logistic regression for sentiment analysis using the method taught in this week's lecture, you have to:
   - Performing data processing.
   - Create a dictionary that maps the word and the class that word is found in to the number of times that word is found in the class.
   - For each tweet, you have to create a  positive feature with the sum of positive counts of each word in that tweet. You also have to create a negative feature with the sum of negative counts of each word in that tweet.

7. When training logistic regression, you have to perform the following operations in the desired order.
   - Initialize parameters, classify/predict, get gradient, update, get loss, repeat

8. Assuming we got the classification correct, where $y^{(i)} = 1$ for some specific example $i$, this means that $h(x^{(i)}, \theta) \geq 0.5$. Which of the following has to hold:
   - Our prediction, $h(x^{(i)}, \theta)$ for this specific training example is greater than $(1 - h(x^{(i)}, \theta))$.

9. What is the purpose of gradient descent? Select all that apply.
    - Gradient descent allows us to learn the parameters θ in logistic regression as to minimize the loss function J.
    - Gradient descent,  grad_theta allows us to update the parameters θ by computing θ=θ−α∗grad_theta

10. What is a good metric that allows you to decide when to stop training/trying to get a good model? Select all that apply.
    - When you plot the cost versus (# of iterations) and you see that your the loss is converging (i.e. no longer changes as much).
    - When your accuracy is good enough on the test set.
