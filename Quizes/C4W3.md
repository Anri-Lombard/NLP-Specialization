# Question Answering

1. The English wikipedia is about 13 GB. The T5 model, that you will be working with is trained on the C4 corpus, which is how many GB?
   - 800 GB
2. Which of the following are true about pre-training in NLP?
   - It allows you to get better results.
   - It allows you to use information learned from a different task while working on a specific task.
   - It speeds training.
3. What is fine-tuning in NLP?
   - Fine tuning means taking existing weights of deeplearning model, and tweaking them a little bit to get a desired output, usually better results, on some specific task.
4. Select all that apply for Masked Language Modeling. (MLM)
   - The cross entropy loss over V classes is used when doing the prediction.
   - The goal is to predict the masked token.
   - Choose 15% of the tokens at random: mask them 80% of the time, replace them with a random token 10% of the time, or keep as is 10% of the time.
5. What does the BERT objective consist of?
   - It consists of the sum of a binary loss used for next sentence prediction and a cross entropy loss over V tokens used for the masked language modeling.
6. Which of the following inputs could be used for the BERT model?
   - All of the above
7. How does the prefix language model attention work in the T5 model?
   - It uses bidirectional attention for the inputs (i.e. X's) and causal attention mapping the outputs (Y's ) at time t, to all the previous X's and outputs before timestep t.
8. When training these latest NLP models, you end up training a model that can do many tasks. For example, you usually have data for sentiments, QA, chatbot, summarization, etc. The question now is how do you combine the datasets using temperature scaled mixing?
   - You will adjust the “temperature” of the mixing rates. This temperature parameter allows you to weight certain examples more than others. When T = 1, this approach is equivalent to examples-proportional mixing and as T increases the proportions become closer to equal mixing.
9. When doing fine-tuning, how do adapter layers work?
    - It allows you to add a new layer and then you only fine-tune the new layer you added.
10. Which of the following is not evaluated using the GLUE benchmark?
    - Machine Translation
