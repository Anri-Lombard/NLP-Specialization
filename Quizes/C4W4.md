# Chatbot

1. Which of the following are issues with transformers?
   - Attention on sequence of length $L$ takes $L^2$ time and memory.
   - N layers take N times as much memory.
2. Why do we need to store activations somewhere when implementing the transformer network?
   - We need to save them to compute the back-propagation.
3. Why do we use locality sensitive hashing when computing attention?
   - It allows us to not have to compare each query with each key. Instead we only compare the vectors that are found in the same bucket.
   - It is a faster way to compute attention.
4. What is the point of using reversible layers?
   - It allows you to reconstruct the the activations and as a result you do not have to save them.
5. How would you recompute $x_2$?
   - $x_2=y_2 - FF(y_1)$
6. Select two main components that the reformer uses which makes it more efficient than the transformers.
   - Locality sensitive hashing
   - Reversible layers
7. What are the pros and cons of having more hashes when implementing LSH?
   - The more hashes you have the more  accurate your model is, but the slower it is.
8. How many words can a reformer hold on a single 16GB GPU?
   - 1 million
9. In LSH, you want to attend to a bucket in a previous chunk because it covers the case with a hash bucket that is split over more than 1 chunk.
    - True
10. One reason, according to the lecture why the BLEU score for Reversible Transformers is slightly better than the original Transformers, is due to parameter tuning in the 3 years since the original Transformer paper was published.
    - True
