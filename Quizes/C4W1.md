# Neural Machine Translation

1. Which of the following are bottlenecks when implementing seq2seq models?
   - You are trying to store variable length sequences in a fixed memory, for example, you are trying to store articles of different lengths in a fixed 100 dimensional vector.
   - There are vanishing/exploding gradient problems.
2. What are some of the benefits of using attention?
   - It allows you to focus on the parts that matter more.
   - It helps with the information bottleneck issue.
3. What are the major components in the attention mechanism that are required? Select all that apply.
   - Queries
   - Keys
   - Values
   - Softmax√•
4. Which sentinel is used in lecture to represent the end of sentence token in machine translation?
   - 1
5. Teacher forcing  uses the actual output from the training dataset at time step $y^{(t)}$ as input in the next time step $X^{(t+1)}$, instead of the output generated by your model.
   - True
6. The BLEU score's range is as follows:
   - The closer to 0, the worse it is, the closer to 1, the better it is.
7. BLEU (Vanilla Implementation) is defined as:
   - (Sum of unique n-gram counts, overlapping in the candidate and reference) / (Total # of n-grams in the candidate)
8. What is the difference between precision and recall in Rouge?
   - Precision is defined as:

    (Sum of overlapping unigrams in model and reference)/(total # of words in model)

    Recall is defined as:

    (Sum of overlapping unigrams in model and reference)/(total # of words in reference)
9. Greedy decoding
   - Allows you select the word with the highest probability at each time step.
10. When implementing Minimum Bayes Risk method in decoding, let's say with 4 samples, you have to implement the following.
    - True
